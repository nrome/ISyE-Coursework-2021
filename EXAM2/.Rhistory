sim = diag(.5,5, nrow = 5)
#   sim[upper.tri(sim)] = c(0.1, 0.41, 0.64, 0.55, 0.47, 0.44, 0.35, 0.98, 0.85, 0.76)
sim[upper.tri(sim)] = c(0.34, 0.84, 0.36, 0.25, 0.52, 0.71, 0.1, 0.94, 0.81, 0.65)
sim = sim + t(sim)
sim
#   Single Link
agn = agnes(sim, diss = F, method = "single")
#   AGNES
agn = agnes(dist, method = "single")
#   Load packages with pacman
pacman::p_load(pacman, animation, cluster)
#   Single Link
agn = agnes(sim, diss = F, method = "single")
plot(agn)
#   Complete Link
agn = agnes(sim, diss = F, method = "complete")
plot(agn)
#   Load packages with pacman
pacman::p_load(pacman, animation, cluster)
D = matrix(c(2, 7, 4, 5, 8, 3, 7, 2, 5, 6, 4, 4, 3, 1, 8, 7), ncol = 2, byrow = TRUE)
#   Represent eight datasets
rownames(D) = c('A1', 'A2', 'A3', 'B1', 'B2', 'B3', 'C1', 'C2')
#   Denote x and y coordinates
colnames(D)  = c('x', 'y')
#   Three Clusters A1, B1, C1 centers
center = D[c('A1', 'B1', 'C1'),]
center
#   Distance function is Euclidean distance
dissim = dist(rbind(D, center), method = 'euclidean')
#   Convert to conventional matrix
dissim = as.matrix(dissim)
dissim
#   8 points
npoint = nrow(D)
#   first eight rows and all columns except first eight columns
dissim = dissim[1:npoint, -(1:npoint)]
#   group by min value - returns columns w/ smallest val w/in each row
group = apply(dissim, MARGIN = 1, FUN = which.min)
#   Use the k-means algorithm to show only
#   Assignment step:
kmeans.group = function(D, center) {
npoint = nrow(D)
dissim = dist(rbind(D, center), method = 'euclidean')
dissim = as.matrix(dissim)
dissim = dissim[1:npoint, -(1:npoint)]
group = apply(dissim, MARGIN = 1, FUN = which.min)
return(group)
}
#   test group
kmeans.group(D, center) == group
#   Update step:
kmeans.newcenter = function(D, group){
center.x = aggregate(D[,1], by = list(group), FUN = mean)
center.y = aggregate(D[,2], by = list(group), FUN = mean)
center = cbind(center.x$x, center.y$x)
return(center)
}
#   test center
kmeans.newcenter(D, group) == center
#   Calculate using center points
#   first execution
group = kmeans.group(D, center)
plot(D, col = group)
group
group
plot(D, col = group)
#   Calculate new center points - final points where groups no longer change/converge
center = kmeans.newcenter(D, group)
points(center, col = 1:3, pch = 8)
group = kmeans.group(D, center)
group
#   Clean environment
rm(list = ls())
#   Remove plots
dev.off()  # do only if a plot exists
#   Clear the console
cat("\014")   # command shortcut ctrl+L
D = matrix(c(2, 7, 4, 5, 8, 3, 7, 2, 5, 6, 4, 4, 3, 1, 8, 7), ncol = 2, byrow = TRUE)
#   Represent eight datasets
rownames(D) = c('A1', 'A2', 'A3', 'B1', 'B2', 'B3', 'C1', 'C2')
#   Denote x and y coordinates
colnames(D)  = c('x', 'y')
#   Three Clusters A1, B1, C1 centers
center = D[c('A1', 'B1', 'C1'),]
#   Distance function is Euclidean distance
dissim = dist(rbind(D, center), method = 'euclidean')
#   Convert to conventional matrix
dissim = as.matrix(dissim)
#   8 points
npoint = nrow(D)
#   first eight rows and all columns except first eight columns
dissim = dissim[1:npoint, -(1:npoint)]
#   group by min value - returns columns w/ smallest val w/in each row
group = apply(dissim, MARGIN = 1, FUN = which.min)
#   Use the k-means algorithm to show only
#   Assignment step:
kmeans.group = function(D, center) {
npoint = nrow(D)
dissim = dist(rbind(D, center), method = 'euclidean')
dissim = as.matrix(dissim)
dissim = dissim[1:npoint, -(1:npoint)]
group = apply(dissim, MARGIN = 1, FUN = which.min)
return(group)
}
#   Update step:
kmeans.newcenter = function(D, group){
center.x = aggregate(D[,1], by = list(group), FUN = mean)
center.y = aggregate(D[,2], by = list(group), FUN = mean)
center = cbind(center.x$x, center.y$x)
return(center)
}
#   Calculate using center points - first execution
group = kmeans.group(D, center)
group
plot(D, col = group)
#   Calculate new center points - final points where groups no longer change/converge
center = kmeans.newcenter(D, group)
points(center, col = 1:3, pch = 8)
group = kmeans.group(D, center)
group
#   R kmeans package ####################
D
cl = kmeans(D, centers = D[c('A1', 'B1', 'C1'),], algorithm = "MacQueen")
print(cl)
#   exact same from manual/user defined function - final points
plot(D, col = cl$cluster)
#   Set working directory
setwd("/Users/nickromero/Desktop/UW-MADISON-ENG/ENGINEERING-412/EXAM2")
p.3.dataset = as.data.frame(read_excel('Uni6p2.xlsx'))
#   Load packages with pacman
pacman::p_load(pacman, rpart, rpart.plot, readxl, e1071, animation, cluster)
p.3.dataset = as.data.frame(read_excel('Uni6p2.xlsx'))
print(p.e.dataset)
print(p.3.dataset)
print(car)
car = as.data.frame(read_excel('Uni6p2.xlsx'))
print(car)
#   Set working directory
setwd("/Users/nickromero/Desktop/UW-MADISON-ENG/ENGINEERING-412/HW5/Resources")
car = as.data.frame(read_excel('Uni6p2.xlsx'))
print(car)
#   Clean environment
rm(list = ls())
#   Remove plots
dev.off()  # do only if a plot exists
#   Clear the console
cat("\014")   # command shortcut ctrl+L
#   Load packages with pacman
pacman::p_load(pacman, rpart, rpart.plot, readxl, e1071, animation, cluster)
#   Set working directory
setwd("/Users/nickromero/Desktop/UW-MADISON-ENG/ENGINEERING-412/EXAM2")
problem.3.dataset = as.data.frame(read_excel('Uni6p2.xlsx'))
print(problem.3.dataset)
model = naiveBayes(Class ~ A + B + C, data = problem.3.dataset)
model
#   Use the 10th row as the test set
predict(model, newdata = problem.3.dataset[10, ], type = "raw") #   No: 0.1818182 < Yes: 0.8181818 [YES]
predict(model, newdata = problem.3.dataset[10, ]) #   Getting a factor(0) and not a 'YES' result
#   Clean environment
rm(list = ls())
#   Remove plots
dev.off()  # do only if a plot exists
#   Clear the console
cat("\014")   # command shortcut ctrl+L
problem.3.dataset = as.data.frame(read_excel('Uni6p2.xlsx'))
#   Data set Col -> Class [+] is P, [-] is N)
print(problem.3.dataset)
model = naiveBayes(Class ~ A + B + C, data = problem.3.dataset)
model
#   Use the 10th row as the test set
predict(model, newdata = problem.3.dataset[10, ], type = "raw") #   No: 0.1818182 < Yes: 0.8181818 [YES]
problem.3.dataset = as.data.frame(read_excel('Uni6p2.xlsx'))
#   Clean environment
rm(list = ls())
#   Remove plots
dev.off()  # do only if a plot exists
#   Clear the console
cat("\014")   # command shortcut ctrl+L
problem.3.dataset = as.data.frame(read_excel('Uni6p2.xlsx'))
#   Data set Col -> Class [+] is P, [-] is N)
print(problem.3.dataset)
model = naiveBayes(Class ~ A + B + C, data = problem.3.dataset)
model
#   Use the 10th row as the test set
predict(model, newdata = problem.3.dataset[10, ], type = "raw") #   [-]: 0.998999 > [+]: 0.001000985
problem.1.dataset = as.data.frame(read_excel('Uni6p1.xlsx'))
m1 <- rpart(A ~., data = problem.1.dataset, parms = list(split = 'information'))
m2 <- rpart(B ~., data = problem.1.dataset, parms = list(split = 'information'))
#   nb.model <- naiveBayes(status ~., data = problem.1.dataset, parms = list(split = 'information'))
print(m1)
summary(m1)
rpart.plot(m1,extra = 1)
m2 <- rpart(B ~., data = problem.1.dataset, parms = list(split = 'information'))
print(m2)
summary(m2)
rpart.plot(m2,extra = 1)
entropy(problem.1.dataset$A)
#   Load packages with pacman
pacman::p_load(pacman, rpart, rpart.plot, readxl, e1071, animation, cluster, entropy)
entropy(problem.1.dataset$A)
#   Clean environment
rm(list = ls())
#   Remove plots
dev.off()  # do only if a plot exists
#   Clear the console
cat("\014")   # command shortcut ctrl+L
problem.3.dataset = as.data.frame(read_excel('Uni6p2.xlsx'))
#   Data set Col -> Class [+] is P, [-] is N)
print(problem.3.dataset)
#   Set working directory
setwd("/Users/nickromero/Desktop/UW-MADISON-ENG/ENGINEERING-412/EXAM2")
problem.3.dataset = as.data.frame(read_excel('Uni6p2.xlsx'))
#   Load packages with pacman
pacman::p_load(pacman, rpart, rpart.plot, readxl, e1071, animation, cluster)
#   Clear the console
cat("\014")   # command shortcut ctrl+L
#   Load packages with pacman
pacman::p_load(pacman, rpart, rpart.plot, readxl, e1071, animation, cluster)
problem.3.dataset = as.data.frame(read_excel('Uni6p2.xlsx'))
#   Data set Col -> Class [+] is P, [-] is N)
print(problem.3.dataset)
model = naiveBayes(Class ~ A + B + C, data = problem.3.dataset)
model
#   Use the 10th row as the test set
predict(model, newdata = problem.3.dataset[10, ], type = "raw") #   [-]: 0.998999 > [+]: 0.001000985
problem.1.dataset = read.csvl('binaryclass.csv')
#   Load packages with pacman
pacman::p_load(pacman, rpart, rpart.plot, readxl, readcsv, e1071, animation, cluster)
problem.1.dataset = read.csvl('binaryclass.csv')
problem.1.dataset = read.csv('binaryclass.csv')
setwd("~/Desktop/UW-MADISON-ENG/ENGINEERING-412/EXAM2")
#   Set working directory
setwd("~/Desktop/UW-MADISON-ENG/ENGINEERING-412/EXAM2")
problem.1.dataset = read.csv('binaryclass.csv')
problem.1.dataset = read.csv('binaryclass.csv')
problem.1.dataset = read.csv('binaryclass.csv')
problem.1.dataset
#   Clean environment
rm(list = ls())
#   Uninstall packages
p_unload(all)
#   Remove plots
dev.off()  # do only if a plot exists
#   Clear the console
cat("\014")   # command shortcut ctrl+L
#   Load packages with pacman
pacman::p_load(pacman, rpart, rpart.plot, readxl, readcsv, e1071, animation, cluster)
#   Set working directory
setwd("~/Desktop/UW-MADISON-ENG/ENGINEERING-412/EXAM2")
problem.1.dataset = read.csv('binaryclass.csv')
problem.1.dataset
#   Load packages with pacman
pacman::p_load(pacman, rpart, rpart.plot, readxl, e1071, animation, cluster)
problem.1.dataset = read.csv('binaryclass.csv')
problem.1.dataset
#   Clear the console
cat("\014")   # command shortcut ctrl+L
head(problem.1.dataset)
str(problem.1.dataset)
summary(problem.1.dataset)
problem.1.dataset$count = 1
entropy = function(x) {
# x is a sequence of frequencies such as x = c(5,9)
pi = x/sum(x)
info = -sum(pi * log(pi, bbase = 2))
return(info)
}
entropy = function(x) {
# x is a sequence of frequencies such as x = c(5,9)
pi = x/sum(x)
info = -sum(pi * log(pi, base = 2))
return(info)
}
x = c(6,4)
entropy(x)
split.A = aggregate(problem.1.dataset$count, by = problem.1.dataset[c('A','Class.Label')], FUN = sum)
# apply the sum() function to play$count for each group categorized according to Outlook and PlayTennis
split.A # the last column is the count
Dj = aggregate(split.A$x, by = split.A['A'], FUN = sum)
InfoDj = aggregate(split.A$x, by = split.A['A'], FUN = entropy)
A.info = sum(Dj$x/sum(Dj$x) * InfoDj$x)
A.info
Info = function(count, by, status, data){
# count_col is the column name for the frequency
# by_col is the column name defining the group
# target_col is the column name defining the target
# data is a data.frame
# for example, data = play, count_col = "count", by_col = "Outlook", target_col = "PlayTennis"
Split = aggregate(data[[count]], data[c(by, status)], FUN = sum)
Dj = aggregate(Split$x, Split[by], FUN = sum)
InfoDj = aggregate(Split$x, Split[by], FUN = entropy)
info = sum(Dj$x/sum(Dj$x) * InfoDj$x)
return(info)
}
Info('count', 'A', 'Class.Label', problem.1.dataset)
Info('count', 'B', 'Class.Label', problem.1.dataset)
Info.gain = function(count_col, by_col, target_col, data){
info.nosplit = Info(count_col, NULL, target_col, data)
info.split = Info(count_col, by_col, target_col, data)
gain = info.nosplit - info.split
return(gain)
}
Info.gain('count', 'A', 'Class.Label', problem.1.dataset)
Info.gain('count', 'B', 'Class.Label', problem.1.dataset)
#   Clean environment
rm(list = ls())
#   Clear the console
cat("\014")   # command shortcut ctrl+L
problem.1.dataset = read.csv('binaryclass.csv')
head(problem.1.dataset)
str(problem.1.dataset)
summary(problem.1.dataset)
problem.1.dataset$count = 1
entropy = function(x) {
# x is a sequence of frequencies such as x = c(5,9)
pi = x/sum(x)
info = -sum(pi * log(pi, base = 2))
return(info)
}
x = c(6,4)
#   Overall Entropy
entropy(x)
Info = function(count, by, status, data){
Split = aggregate(data[[count]], data[c(by, status)], FUN = sum)
Dj = aggregate(Split$x, Split[by], FUN = sum)
InfoDj = aggregate(Split$x, Split[by], FUN = entropy)
info = sum(Dj$x/sum(Dj$x) * InfoDj$x)
return(info)
}
Info('count', 'A', 'Class.Label', problem.1.dataset)
Info('count', 'B', 'Class.Label', problem.1.dataset)
Info.gain = function(count_col, by_col, target_col, data){
info.nosplit = Info(count_col, NULL, target_col, data)
info.split = Info(count_col, by_col, target_col, data)
gain = info.nosplit - info.split
return(gain)
}
Info.gain('count', 'A', 'Class.Label', problem.1.dataset)
Info.gain('count', 'B', 'Class.Label', problem.1.dataset)
problem.1.dataset
#   Clear the console
cat("\014")   # command shortcut ctrl+L
problem.1.dataset
#   Overall Entropy
entropy(x)
Info('count', 'A', 'Class.Label', problem.1.dataset)
Info('count', 'B', 'Class.Label', problem.1.dataset)
Info.gain('count', 'A', 'Class.Label', problem.1.dataset)
Info.gain('count', 'B', 'Class.Label', problem.1.dataset)
#   Clear the console
cat("\014")   # command shortcut ctrl+L
str(problem.1.dataset)
summary(problem.1.dataset)
#   Clear the console
cat("\014")   # command shortcut ctrl+L
problem.1.dataset
str(problem.1.dataset)
summary(problem.1.dataset)
#   Overall Entropy
entropy(x)
Info('count', 'A', 'Class.Label', problem.1.dataset)
Info('count', 'B', 'Class.Label', problem.1.dataset)
Info.gain('count', 'A', 'Class.Label', problem.1.dataset)
Info.gain('count', 'B', 'Class.Label', problem.1.dataset)
